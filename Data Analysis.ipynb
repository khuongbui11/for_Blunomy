{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:34: DtypeWarning: Columns (10,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2006_2020 = pd.read_csv(path_2006_2020)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:34: DtypeWarning: Columns (6,9,10,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2006_2020 = pd.read_csv(path_2006_2020)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0,3,11,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:34: DtypeWarning: Columns (3,25,26,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2006_2020 = pd.read_csv(path_2006_2020)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n",
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/2748065923.py:33: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2000_2005 = pd.read_csv(path_2000_2005)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder paths\n",
    "folder_2000_2005 = \"2000 to 2005 ACCIDENT\"  # Replace with the actual path\n",
    "folder_2006_2020 = \"Accident\"  # Replace with the actual path\n",
    "\n",
    "# List of CSV files to merge\n",
    "csv_files = [\n",
    "    \"accident\",\n",
    "    \"person\",\n",
    "    \"vehicle\",\n",
    "    \"accident_event\",\n",
    "    \"ACCIDENT_LOCATION\",\n",
    "    \"road_surface_cond\",\n",
    "    \"atmospheric_cond\",\n",
    "    \"subdca\",\n",
    "    \"accident_chainage\",\n",
    "    \"Node\",\n",
    "    \"NODE_ID_COMPLEX_INT_ID\"\n",
    "]\n",
    "\n",
    "# Dictionary to hold merged dataframes\n",
    "merged_data = {}\n",
    "\n",
    "# Loop through each CSV file and merge\n",
    "for csv_file in csv_files:\n",
    "    # Construct the full path for each file in both folders\n",
    "    path_2000_2005 = os.path.join(folder_2000_2005, csv_file + \".csv\")\n",
    "    path_2006_2020 = os.path.join(folder_2006_2020, csv_file + \".csv\")\n",
    "    \n",
    "    # Read the CSVs\n",
    "    df_2000_2005 = pd.read_csv(path_2000_2005)\n",
    "    df_2006_2020 = pd.read_csv(path_2006_2020)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    merged_df = pd.concat([df_2000_2005, df_2006_2020], ignore_index=True)\n",
    "    \n",
    "    # Store the merged DataFrame in the dictionary\n",
    "    merged_data[csv_file] = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert ACCIDENTDATE to datetime format\n",
    "merged_data[\"accident\"]['ACCIDENTDATE'] = pd.to_datetime(merged_data[\"accident\"]['ACCIDENTDATE'], dayfirst=True)\n",
    "\n",
    "# Remove data for years 2000 and 2020\n",
    "filtered_data_1 = merged_data[\"accident\"][~merged_data[\"accident\"]['ACCIDENTDATE'].dt.year.isin([2000, 2020])]\n",
    "\n",
    "# 2. Change the label of Severity\n",
    "severity_mapping = {\n",
    "    1: 'Fatal accident',\n",
    "    2: 'Serious injury accident',\n",
    "    3: 'Other injury accident',\n",
    "    4: 'Non injury accident'\n",
    "}\n",
    "filtered_data_1['SEVERITY'] = filtered_data_1['SEVERITY'].map(severity_mapping)\n",
    "\n",
    "# Group by month and severity and count the number of accidents\n",
    "yearly_accidents = (filtered_data_1.groupby([filtered_data_1['ACCIDENTDATE'].dt.to_period(\"Y\"), 'SEVERITY'])\n",
    "                     .size()\n",
    "                     .unstack()\n",
    "                     .fillna(0))\n",
    "\n",
    "# Convert Period index back to datetime format for plotting\n",
    "yearly_accidents.index = yearly_accidents.index.to_timestamp()\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "for severity, values in yearly_accidents.items():\n",
    "    # Check if there are any non-zero values for the severity\n",
    "    if values.sum() > 0:\n",
    "        plt.plot(values, label=f'Severity: {severity}')\n",
    "\n",
    "plt.title('Number of Accidents Over Time by Severity', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.ylabel('Number of Accidents', fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data based on the specified years\n",
    "filtered_data_2 = merged_data[\"accident\"][~merged_data[\"accident\"]['ACCIDENTDATE'].dt.year.isin([2000, 2005, 2006, 2020])]\n",
    "\n",
    "# Grouping by month and severity to calculate the number of accidents for each month\n",
    "monthly_crash_frequency = filtered_data_2.groupby([filtered_data_2['ACCIDENTTIME'].dt.hour, 'SEVERITY']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "monthly_crash_frequency.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis')\n",
    "\n",
    "plt.title('Monthly Crash Frequency by Severity (Excluding years 2000, 2005, 2006, 2020)', fontsize=18)\n",
    "plt.xlabel('Month', fontsize=16)\n",
    "plt.ylabel('Number of Accidents', fontsize=16)\n",
    "plt.xticks(ticks=range(24), rotation=0)\n",
    "plt.legend(fontsize=14,loc='upper center', ncol=4)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data based on the specified years\n",
    "filtered_data_3 = merged_data[\"accident\"]\n",
    "\n",
    "# Function to convert time to hour\n",
    "def convert_time(time):\n",
    "    if isinstance(time, str):  # Check if the value is a string\n",
    "        time = time.strip()  # Remove any extra spaces\n",
    "        try:\n",
    "            return pd.to_datetime(time, format='%H:%M:%S').hour\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "filtered_data_3['ACCIDENTTIME'] = filtered_data_3['ACCIDENTTIME'].apply(convert_time)\n",
    "\n",
    "# Grouping by hour and severity to calculate the number of accidents for each hour\n",
    "hourly_crash_frequency = filtered_data_3.groupby([filtered_data_3['ACCIDENTTIME'], 'SEVERITY']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "hourly_crash_frequency.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis')\n",
    "\n",
    "plt.title('Hourly Crash Frequency by Severity', fontsize=18)\n",
    "plt.xlabel('Hour', fontsize=16)\n",
    "plt.ylabel('Number of Accidents', fontsize=16)\n",
    "plt.xticks(ticks=range(24), labels=[str(i) for i in range(24)], rotation=0)\n",
    "plt.legend(fontsize=14,loc='upper center', ncol=4)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e64ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Day of Week and severity to calculate the number of accidents for each day\n",
    "daily_crash_frequency = filtered_data_2.groupby([filtered_data_2['Day Week Description'], 'SEVERITY']).size().unstack().fillna(0)\n",
    "\n",
    "# Define an ordered category for the days of the week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_crash_frequency = daily_crash_frequency.reindex(day_order)\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "daily_crash_frequency.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis')\n",
    "\n",
    "plt.title('Daily Crash Frequency by Severity', fontsize=18)\n",
    "plt.xlabel('Day', fontsize=18)\n",
    "plt.ylabel('Number of Accidents', fontsize=18)\n",
    "plt.xticks(ticks=range(7), fontsize=16, rotation=0)\n",
    "plt.legend(fontsize=14,loc='upper left', ncol=2)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bc297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the \"Accident Type Desc\" column and count the occurrences\n",
    "accident_type_counts = merged_data[\"accident\"]['Accident Type Desc'].value_counts()\n",
    "\n",
    "# Filter the accident types to only include those with a proportion greater than 1%\n",
    "filtered_accident_type_counts = accident_type_counts[accident_type_counts / accident_type_counts.sum() > 0.01]\n",
    "\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "filtered_accident_type_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=sns.color_palette('viridis', len(accident_type_counts)))\n",
    "plt.title('Distribution of Accident Types (Greater than 1%)', fontsize=18)\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de373c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Accident dataframe with the Node dataframe on the \"ACCIDENT_NO\" column to get the \"POSTCODE_NO\" column\n",
    "merged_df = merged_data[\"accident\"].merge(merged_data[\"Node\"][['ACCIDENT_NO', 'POSTCODE_NO', 'Lat','Long','LGA_NAME']], on='ACCIDENT_NO', how='left')\n",
    "\n",
    "# Renaming the POSTCODE_NO column to POSTCODE for clarity\n",
    "merged_df.rename(columns={'POSTCODE_NO': 'POSTCODE'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the merged dataframe to confirm\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the suburb dataset from the provided Excel file\n",
    "suburb_df = pd.read_excel('suburb.xlsx')\n",
    "\n",
    "# Merging the merged_df with the suburb_df on the \"POSTCODE\" column to get the corresponding suburb (locality)\n",
    "merged_df_with_suburb = merged_df.merge(suburb_df, left_on='POSTCODE', right_on='postcode', how='left')\n",
    "\n",
    "# Picking only the first suburb for each accident record\n",
    "merged_df_with_suburb = merged_df_with_suburb.drop_duplicates(subset='ACCIDENT_NO', keep='first')\n",
    "\n",
    "# Renaming the locality column to SUBURB for clarity\n",
    "merged_df_with_suburb.rename(columns={'locality': 'SUBURB'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the merged dataframe to confirm\n",
    "merged_df_with_suburb[['ACCIDENT_NO', 'POSTCODE', 'SUBURB']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b183e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by LGA_NAME and counting the number of accidents\n",
    "lga_accident_counts = merged_df.groupby('LGA_NAME').size().sort_values(ascending=False)\n",
    "\n",
    "# Displaying the top 10 LGA_NAMEs with the most accidents\n",
    "top_10_lga = lga_accident_counts.head(10)\n",
    "top_10_lga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy dataframe\n",
    "merged_df_2 = merged_data[\"accident\"]\n",
    "\n",
    "# Mapping the severity values to their respective descriptions\n",
    "severity_mapping = {\n",
    "    1: 'Fatal accident',\n",
    "    2: 'Serious injury accident',\n",
    "    3: 'Other injury accident',\n",
    "    4: 'Non injury accident'\n",
    "}\n",
    "merged_df_2['SEVERITY'] =  merged_df_2['SEVERITY'].map(severity_mapping)\n",
    "\n",
    "# Creating a crosstab between 'Light Condition Desc' and 'SEVERITY'\n",
    "cross_tab = pd.crosstab(merged_df_2['Light Condition Desc'], merged_df_2['SEVERITY'])\n",
    "\n",
    "# Convert counts to percentages by dividing each column by the sum of that column\n",
    "cross_tab_percentage = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cross_tab_percentage, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Relationship between Light Condition and Severity', fontsize=16)\n",
    "plt.xlabel('Severity', fontsize=14)\n",
    "plt.ylabel('Light Condition', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a crosstab between 'Light Condition Desc' and 'SEVERITY'\n",
    "cross_tab_speedzone = pd.crosstab(filtered_speed_zone['SPEED_ZONE'], filtered_speed_zone['SEVERITY'])\n",
    "\n",
    "# Convert counts to percentages by dividing each column by the sum of that column\n",
    "cross_tab_percentage_speedzone = cross_tab_speedzone.div(cross_tab_speedzone.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cross_tab_percentage_speedzone, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Relationship between Speed Zone and Severity', fontsize=16)\n",
    "plt.xlabel('Severity', fontsize=14)\n",
    "plt.ylabel('Speed zone', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b28df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Accident dataframe with the Atmospheric_cond dataframe on the \"ACCIDENT_NO\" column to get the 'Atmosph Cond Desc' column\n",
    "merged_df_Atmospheric = pd.merge(merged_data[\"accident\"], merged_data[\"atmospheric_cond\"][['ACCIDENT_NO', 'Atmosph Cond Desc']], on='ACCIDENT_NO', how='inner')\n",
    "\n",
    "# Display the first few rows of the merged dataframe to confirm\n",
    "merged_df_Atmospheric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96525f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a crosstab between 'Atmosph Cond Desc' and 'SEVERITY'\n",
    "cross_tab = pd.crosstab(merged_df_Atmospheric['Atmosph Cond Desc'], merged_df_Atmospheric['SEVERITY'])\n",
    "\n",
    "# Convert counts to percentages by dividing each column by the sum of that column\n",
    "cross_tab_percentage = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cross_tab_percentage, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Relationship between Atmospheric Condition and Severity', fontsize=16)\n",
    "plt.xlabel('Severity', fontsize=14)\n",
    "plt.ylabel('Light Condition', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20226a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Accident dataframe with the Atmospheric_cond dataframe on the \"ACCIDENT_NO\" column to get the 'Surface Cond Desc' column\n",
    "merged_df_Surface = pd.merge(merged_data[\"accident\"], merged_data[\"road_surface_cond\"][['ACCIDENT_NO', 'Surface Cond Desc']], on='ACCIDENT_NO', how='inner')\n",
    "\n",
    "# Display the first few rows of the merged dataframe to confirm\n",
    "merged_df_Surface.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7329500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a crosstab between 'Atmosph Cond Desc' and 'SEVERITY'\n",
    "cross_tab = pd.crosstab(merged_df_Surface['Surface Cond Desc'], merged_df_Surface['SEVERITY'])\n",
    "\n",
    "# Convert counts to percentages by dividing each column by the sum of that column\n",
    "cross_tab_percentage = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cross_tab_percentage, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Relationship between Surface Condition and Severity', fontsize=16)\n",
    "plt.xlabel('Severity', fontsize=14)\n",
    "plt.ylabel('Light Condition', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ac60ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buida\\AppData\\Local\\Temp/ipykernel_21356/3339354653.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  merged_df_3['ACCIDENTTIME'] = merged_df_3['ACCIDENTTIME'].str.replace('.', ':')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HOUR  DAY_OF_WEEK  LIGHT_CONDITION  SPEED_ZONE  SURFACE_COND  \\\n",
      "102694   0.0            1                3          60           1.0   \n",
      "102695  19.0            6                1          60           1.0   \n",
      "102696  15.0            5                1          60           1.0   \n",
      "102697  12.0            2                1          60           1.0   \n",
      "102698  19.0            5                1          60           1.0   \n",
      "\n",
      "        ATMOSPH_COND   LGA_NAME  SEVERITY  \n",
      "102694           1.0      YARRA         2  \n",
      "102695           1.0      YARRA         3  \n",
      "102696           1.0  MELBOURNE         3  \n",
      "102697           1.0  MELBOURNE         3  \n",
      "102698           1.0      YARRA         3  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Merge with Road_Surface_Cond table\n",
    "merged_df_1 = pd.merge(merged_data[\"accident\"], merged_data[\"road_surface_cond\"][['ACCIDENT_NO', 'SURFACE_COND']], on='ACCIDENT_NO', how='left')\n",
    "\n",
    "# Merge with Atmospheric_Cond table\n",
    "merged_df_2 = pd.merge(merged_df_1, merged_data[\"atmospheric_cond\"][['ACCIDENT_NO', 'ATMOSPH_COND']], on='ACCIDENT_NO', how='left')\n",
    "\n",
    "# Merge with Node table\n",
    "merged_df_3 = pd.merge(merged_df_2, merged_data[\"Node\"][['ACCIDENT_NO', 'LGA_NAME']], on='ACCIDENT_NO', how='left')\n",
    "\n",
    "# Extract the hour from the `ACCIDENTTIME` column using the correct format and create a new column\n",
    "# Replace \".\" with \":\" in the ACCIDENTTIME column\n",
    "merged_df_3['ACCIDENTTIME'] = merged_df_3['ACCIDENTTIME'].str.replace('.', ':')\n",
    "\n",
    "# Extract the hour\n",
    "merged_df_3['HOUR'] = merged_df_3['ACCIDENTTIME'].str.split(':').str[0]\n",
    "\n",
    "# Convert the HOUR column to numeric\n",
    "merged_df_3['HOUR'] = pd.to_numeric(merged_df_3['HOUR'], errors='coerce')\n",
    "\n",
    "\n",
    "# Filter the dataframe to only include the desired columns\n",
    "final_df = merged_df_3[['HOUR', 'DAY_OF_WEEK', 'LIGHT_CONDITION', 'SPEED_ZONE', 'SURFACE_COND', 'ATMOSPH_COND', 'LGA_NAME', 'SEVERITY']]\n",
    "\n",
    "# Drop rows with missing values\n",
    "cleaned_df = final_df.dropna()\n",
    "\n",
    "# Display the first few rows of the final dataframe\n",
    "print(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cafe24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOUR               0\n",
      "DAY_OF_WEEK        0\n",
      "LIGHT_CONDITION    0\n",
      "SPEED_ZONE         0\n",
      "SURFACE_COND       0\n",
      "ATMOSPH_COND       0\n",
      "LGA_NAME           0\n",
      "SEVERITY           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"Clean_data.xlsx\"\n",
    "cleaned_df.to_excel(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d9ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
